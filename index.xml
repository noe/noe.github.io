<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Noe Casas on Noe Casas</title>
    <link>/</link>
    <description>Recent content in Noe Casas on Noe Casas</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Noe Casas, 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Chinese-Catalan: A Neural Machine Translation Approach Based on Pivoting and Attention Mechanisms</title>
      <link>/publication/2019_tallip_chinese_catalan/</link>
      <pubDate>Fri, 10 May 2019 10:00:08 +0100</pubDate>
      
      <guid>/publication/2019_tallip_chinese_catalan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Notes on EMNLP 2018</title>
      <link>/post/emnlp2018/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/emnlp2018/</guid>
      <description>

&lt;p&gt;I recently attended the
&lt;a href=&#34;http://emnlp2018.org/&#34; target=&#34;_blank&#34;&gt;Empirical Methods in Natural Language Processing (EMNLP)&lt;/a&gt; conference.
In this post I write about the most remarkable stuff presented there and in the co-located
events, from the point of view of a neural machine translation researcher. These are just
my opinions, feel free to disagree.&lt;/p&gt;

&lt;p&gt;Feedback is very welcome. Please leave your comments as replies to
&lt;a href=&#34;https://twitter.com/noecasas/status/1067365571661049857&#34; target=&#34;_blank&#34;&gt;this tweet&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If, after reading this post, you want to know more about what happened at EMNLP 2018, I recommend
&lt;a href=&#34;https://twitter.com/search?q=%23emnlp2018&amp;amp;src=typd&#34; target=&#34;_blank&#34;&gt;searching for hashtag #emnlp2018 on twitter&lt;/a&gt;
as there was plenty of live tweeting.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Before the main conference, there were two days of co-located events. The three most interesting
ones were the &lt;a href=&#34;http://www.statmt.org/wmt18/&#34; target=&#34;_blank&#34;&gt;Conference on Machine Translation (WMT)&lt;/a&gt;,
the &lt;a href=&#34;http://www.conll.org/2018&#34; target=&#34;_blank&#34;&gt;SIGNLL Conference on Computational Natural Language Learning (CoNLL)&lt;/a&gt;
and the &lt;a href=&#34;https://blackboxnlp.github.io/&#34; target=&#34;_blank&#34;&gt;Blackbox NLP Workshop&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;wmt&#34;&gt;WMT&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.statmt.org/wmt18/&#34; target=&#34;_blank&#34;&gt;WMT&lt;/a&gt; started as a workshop on statistical machine translation, with
several shared task competitions were you could take place it. &lt;a href=&#34;http://talp.upc.edu/&#34; target=&#34;_blank&#34;&gt;We&lt;/a&gt; participated
in the &lt;a href=&#34;http://www.statmt.org/wmt18/translation-task.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;news translation shared task&lt;/strong&gt;&lt;/a&gt;,
specifically in Estonian-English and Finnish-English
(&lt;a href=&#34;http://www.aclweb.org/anthology/W18-6406&#34; target=&#34;_blank&#34;&gt;our system&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;English to German translation, and especially the WMT14 dataset, has lately become the
&lt;a href=&#34;https://nlpprogress.com/english/machine_translation.html&#34; target=&#34;_blank&#34;&gt;standard benchmark&lt;/a&gt; for machine
translation quality (followed by English to French). This year&amp;rsquo;s news translation task included
English to German and Facebook AI Research won in that translation direction with
&lt;a href=&#34;https://research.fb.com/publications/understanding-back-translation-at-scale/&#34; target=&#34;_blank&#34;&gt;their massive use of backtranslation&lt;/a&gt;.
In general, the &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34;&gt;Transformer model&lt;/a&gt; was dominant architecture in the news
translation task, with a special mention to &lt;a href=&#34;https://marian-nmt.github.io/&#34; target=&#34;_blank&#34;&gt;Marian&lt;/a&gt; implementation,
which seems to be gaining momentum due to its
&lt;a href=&#34;https://marian-nmt.github.io/features/#benchmarks&#34; target=&#34;_blank&#34;&gt;very high performance&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can check the full &lt;a href=&#34;http://aclweb.org/anthology/W18-6401.pdf&#34; target=&#34;_blank&#34;&gt;WMT18 news translation task findings report&lt;/a&gt;
for all the details of the competition.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Apart from the news task, this year I found interesting the
&lt;a href=&#34;http://www.statmt.org/wmt18/parallel-corpus-filtering.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;noisy corpus filtering task&lt;/strong&gt;&lt;/a&gt;.
Partitipants were
provided a huge corpus, for which they have to score each sentence pair. Based on the scores, the
organization sampled one smaller and one larger subset and trained statistical and neural MT system
and evaluated the translation quality. I think it&amp;rsquo;s worth mentioning Microsoft&amp;rsquo;s
&lt;a href=&#34;http://aclweb.org/anthology/W18-6478&#34; target=&#34;_blank&#34;&gt;Dual Conditional Cross-Entropy Filtering&lt;/a&gt;, which seemed to be
simple yet very effective.
The full report of the corpus filtering task is &lt;a href=&#34;http://statmt.org/wmt18/pdf/WMT081.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;conll&#34;&gt;CoNLL&lt;/h2&gt;

&lt;p&gt;I could not attend &lt;a href=&#34;http://www.conll.org/2018&#34; target=&#34;_blank&#34;&gt;CoNLL&lt;/a&gt;, but I would like to mention the article
that won the best paper award:
&lt;a href=&#34;http://aclweb.org/anthology/K18-1028&#34; target=&#34;_blank&#34;&gt;Uncovering Divergent Linguistic Information in Word Embeddings with Lessons for Intrinsic and Extrinsic Evaluation&lt;/a&gt;,
which proposes a method to adapt pretrained embedding vectors to work better for
semantics/syntax tasks or for similarity/relatedness tasks.&lt;/p&gt;

&lt;h2 id=&#34;blackbox-nlp&#34;&gt;Blackbox NLP&lt;/h2&gt;

&lt;p&gt;EMNLP stands for &amp;ldquo;Empirical Methods&amp;hellip;&amp;rdquo;. The incorporation of the deep learning black box into NLP has
lead to an emphasis in the empirical part, ruling out most of the interpretability derived from
symbolic approaches and from the modular structure of statistical methods.
Many pieces of research just try to characterize the behaviour of models and the
effects observed when they are subjected to
explorative experiments, but the analyses undergone to explain them are sometimes superficial or
are merely justified by intuition, and there are few conclusions that can be applied to contexts
other than those very specific experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blackboxnlp.github.io/&#34; target=&#34;_blank&#34;&gt;Blackbox NLP Workshop&lt;/a&gt; tried to shed some light on
the inner working of deep NLP models. The workshop gained a lot of attention, with
both the oral sessions and the poster sessions being packed.&lt;/p&gt;

&lt;p&gt;Despite the attempts to make NLP neural networks more interpretable, I think that
the &lt;em&gt;black box nature&lt;/em&gt; of the currently dominant models imposes a hard non-interpretability wall
that prevents us from actually understanding their behaviour completely, and hence we just can
resort to characterizing them under different conditions.
I hope that at some point we devise models with built-in interpretability where
we no longer need to trade interpretability for effectiveness.&lt;/p&gt;

&lt;h2 id=&#34;emnlp-main-conference&#34;&gt;EMNLP: Main Conference&lt;/h2&gt;

&lt;p&gt;Unsupervised Statistical MT techniques were presented by the
&lt;a href=&#34;http://aclweb.org/anthology/D18-1399&#34; target=&#34;_blank&#34;&gt;Basque Country University&lt;/a&gt;
and by &lt;a href=&#34;http://aclweb.org/anthology/D18-1549&#34; target=&#34;_blank&#34;&gt;Facebook AI Research&lt;/a&gt;, achieving
astonishing result without a single pair of parallel sentences.&lt;/p&gt;

&lt;p&gt;The trend from last conferences to try to leverage linguistic knowledge was not
very strong at EMNLP. The most remarkable articles were
&lt;a href=&#34;http://aclweb.org/anthology/D18-1548&#34; target=&#34;_blank&#34;&gt;Linguistically-Informed Self-Attention for Semantic Role Labeling&lt;/a&gt;
and
&lt;a href=&#34;http://aclweb.org/anthology/D18-1492&#34; target=&#34;_blank&#34;&gt;On Tree-Based Neural Sentence Modeling&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cross-lingual learning was present in a lot of different articles
presented at the conference, both regarding word embeddings, machine
translation. You can take a look at the &lt;a href=&#34;https://aclanthology.coli.uni-saarland.de/events/emnlp-2018&#34; target=&#34;_blank&#34;&gt;accepted papers&lt;/a&gt;
and search for &amp;ldquo;cross-lingual&amp;rdquo; to get an idea.&lt;/p&gt;

&lt;p&gt;The recent enthusiams about transfer learning with pretrained models like
AllenNLP&amp;rsquo;s &lt;a href=&#34;https://allennlp.org/elmo&#34; target=&#34;_blank&#34;&gt;ELMo&lt;/a&gt;,
Google&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34;&gt;BERT&lt;/a&gt;,
OpenAI&amp;rsquo;s &lt;a href=&#34;https://blog.openai.com/language-unsupervised/&#34; target=&#34;_blank&#34;&gt;GPT&lt;/a&gt; and
FastAI&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/abs/1801.06146&#34; target=&#34;_blank&#34;&gt;ULMFiT&lt;/a&gt; was not reflected at EMNLP, but will probably do
at next conferences, for which there&amp;rsquo;s still time to build new systems making use of them.&lt;/p&gt;

&lt;h2 id=&#34;summary-highlights&#34;&gt;Summary highlights&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Transformer and backtranslation are the standard machine translation toolbox.&lt;/li&gt;
&lt;li&gt;Cross-lingual and low resource scenarios are gaining momentum.&lt;/li&gt;
&lt;li&gt;Currently, unsupervised SMT works better than unsupervised NMT.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The TALP-UPC Machine Translation Systems for WMT18 News Shared Translation Task</title>
      <link>/publication/2018_wmt_news/</link>
      <pubDate>Mon, 29 Oct 2018 10:00:00 +0000</pubDate>
      
      <guid>/publication/2018_wmt_news/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A differentiable BLEU loss. Analysis and first results</title>
      <link>/publication/2018_iclr/</link>
      <pubDate>Wed, 21 Mar 2018 15:14:10 +0100</pubDate>
      
      <guid>/publication/2018_iclr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>English-Catalan Neural Machine Translation in the Biomedical Domain through the cascade approach</title>
      <link>/publication/2018_lrec/</link>
      <pubDate>Tue, 20 Mar 2018 18:26:08 +0100</pubDate>
      
      <guid>/publication/2018_lrec/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CV</title>
      <link>/cv/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/cv/</guid>
      <description>

&lt;h1 id=&#34;education&#34;&gt;Education&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;MSc in Artificial Intelligence, Spain National Distance Education University (UNED), 2017&lt;/li&gt;
&lt;li&gt;BSc in Computer Science, Polytechnic University of Madrid (UPM), 2005&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;work-experience&#34;&gt;Work experience&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;2017-present: Neural Machine Translation researcher.

&lt;ul&gt;
&lt;li&gt;Company: &lt;a href=&#34;http://www.lucysoftware.com/&#34; target=&#34;_blank&#34;&gt;Lucy Software&lt;/a&gt; (&lt;a href=&#34;https://unitedlanguagegroup.com/&#34; target=&#34;_blank&#34;&gt;United Language Group&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Duties: creation of NMT platform and research on hybridization of rule-based machine translation and NMT.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2016-2017: Data Scientist.

&lt;ul&gt;
&lt;li&gt;Companies: &lt;a href=&#34;http://www.aimsun.com/&#34; target=&#34;_blank&#34;&gt;TSS&lt;/a&gt;, &lt;a href=&#34;http://www.synergicpartners.com/en/&#34; target=&#34;_blank&#34;&gt;Synergic Partners&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Duties included: devise machine learning models and data infrastructure.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2005-2016: Software engineer and Technical Lead.

&lt;ul&gt;
&lt;li&gt;Company: &lt;a href=&#34;http://www.gmv.com/en/&#34; target=&#34;_blank&#34;&gt;GMV&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Duties included: satellite control software development in C++, Java, Python.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;other-skills&#34;&gt;Other skills&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Management and team leading, after leading software development teams of up to 5 people for several years.&lt;/li&gt;
&lt;li&gt;Mentoring, after mentoring several junior software developers.&lt;/li&gt;
&lt;li&gt;Teaching, after teaching different grade and masters courses at two different universities.&lt;/li&gt;
&lt;li&gt;Languages:

&lt;ul&gt;
&lt;li&gt;Spanish - native&lt;/li&gt;
&lt;li&gt;English - full working proficiency&lt;/li&gt;
&lt;li&gt;Chinese - &lt;a href=&#34;https://en.wikipedia.org/wiki/Hanyu_Shuiping_Kaoshi&#34; target=&#34;_blank&#34;&gt;汉语水平考试&lt;/a&gt;4级&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>/teaching/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/teaching/</guid>
      <description>

&lt;hr /&gt;

&lt;h2 id=&#34;2018-2019-introduction-to-deep-learning&#34;&gt;2018-2019. Introduction to Deep Learning&lt;/h2&gt;

&lt;p&gt;Polytechnic University of Catalonia (UPC).&lt;/p&gt;

&lt;p&gt;In charge of project sessions, &lt;a href=&#34;https://telecombcn-dl.github.io/2019-idl&#34; target=&#34;_blank&#34;&gt;including&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linear and logistic regression in Pytorch.&lt;/li&gt;
&lt;li&gt;Multilayer perceptron in Pytorch.&lt;/li&gt;
&lt;li&gt;Multiclass classification and ConvNets in Pytorch.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2018-2019-deep-learning-for-ai&#34;&gt;2018-2019. Deep Learning for AI&lt;/h2&gt;

&lt;p&gt;Polytechnic University of Catalonia (UPC).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://telecombcn-dl.github.io/2018-dlai/&#34; target=&#34;_blank&#34;&gt;Final project instructor&lt;/a&gt;, guiding students
with their final projects, from topic selection, deep learning framework selection,
technical approach to follow, implementation, cloud environment setup, etc.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2017-2018-deep-learning-for-ai&#34;&gt;2017-2018. Deep Learning for AI&lt;/h2&gt;

&lt;p&gt;Polytechnic University of Catalonia (UPC).&lt;/p&gt;

&lt;p&gt;Teaching assistant at the practical sessions, &lt;a href=&#34;https://telecombcn-dl.github.io/2017-dlai/&#34; target=&#34;_blank&#34;&gt;including&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Keras and TensorBoard&lt;/li&gt;
&lt;li&gt;PyTorch&lt;/li&gt;
&lt;li&gt;TensorFlow&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2017-2018-introduction-to-deep-learning&#34;&gt;2017-2018. Introduction to Deep Learning&lt;/h2&gt;

&lt;p&gt;Polytechnic University of Catalonia (UPC).&lt;/p&gt;

&lt;p&gt;In charge of project sessions, &lt;a href=&#34;https://telecombcn-dl.github.io/2018-idl&#34; target=&#34;_blank&#34;&gt;including&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linear and logistic regression in Keras.&lt;/li&gt;
&lt;li&gt;Multilayer perceptron in Keras.&lt;/li&gt;
&lt;li&gt;Multiclass classification and ConvNets.&lt;/li&gt;
&lt;li&gt;Model performance evaluation metrics.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2017-2018-matlab-and-its-applications-in-engineering&#34;&gt;2017-2018. Matlab and its Applications in Engineering&lt;/h2&gt;

&lt;p&gt;Polytechnic University of Catalonia (UPC).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.upc.edu/content/grau/guiadocent/pdf/cat/230206&#34; target=&#34;_blank&#34;&gt;Subject&lt;/a&gt; taught online.
In charge of preparing and grading assingments and final project.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2012-2013-probability-and-statistics&#34;&gt;2012-2013. Probability and Statistics&lt;/h2&gt;

&lt;p&gt;Polytechnic University of Madrid (UPM). Same syllabus as the next entry.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;2011-2012-probability-and-statistics&#34;&gt;2011-2012. Probability and Statistics&lt;/h2&gt;

&lt;p&gt;Polytechnic University of Madrid (UPM)&lt;/p&gt;

&lt;p&gt;Part-time lecturer, &lt;a href=&#34;https://www.fi.upm.es/docs/estudios/grado/1499_2012-13-GUIA-Probabilidades%20y%20Estadistica%20I_1ersemestre.pdf&#34; target=&#34;_blank&#34;&gt;teaching&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Descriptive statistic analysis.&lt;/li&gt;
&lt;li&gt;Foundations of probability theory.&lt;/li&gt;
&lt;li&gt;Random variables, discrete and continuous.&lt;/li&gt;
&lt;li&gt;Confidence intervals.&lt;/li&gt;
&lt;li&gt;Hypothesis testing, parametric and non-parametric.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Notes on EAMT 2018</title>
      <link>/post/eamt2018/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/eamt2018/</guid>
      <description>

&lt;p&gt;I recently attended the
Conference of the &lt;a href=&#34;http://www.eamt.org/&#34; target=&#34;_blank&#34;&gt;European Association for Machine Translation&lt;/a&gt;,
&lt;a href=&#34;http://eamt2018.dlsi.ua.es/&#34; target=&#34;_blank&#34;&gt;EAMT 2018&lt;/a&gt;.
Before this conference I had only attended &lt;a href=&#34;/post/iclr2018/&#34;&gt;ICLR&lt;/a&gt;, which is
an AI conference focused on representation learning, irrespective of the specific
task or paradigm (RL, GANs, NLP, images, etc). On the other hand EAMT is purely
machine translation-focused.&lt;/p&gt;

&lt;p&gt;This year EAMT took place over three days. The first day was focused on research (mostly academia),
the second day on products and projects (industry) and the third one on translators
(translators track).&lt;/p&gt;

&lt;p&gt;One of the recurring topics was &lt;strong&gt;translation quality evaluation&lt;/strong&gt; techniques.
Despite BLEU being the dominant automatic measure and that to some degree it is correlated
with human judgement, it is clear that the machine translation
community is longing for a more grounded quality evaluation measure.
&lt;a href=&#34;http://homepages.inf.ed.ac.uk/bhaddow/&#34; target=&#34;_blank&#34;&gt;Barry Haddow&lt;/a&gt; gave a talk about the
&lt;a href=&#34;http://www.statmt.org/wmt18/&#34; target=&#34;_blank&#34;&gt;WMT competition&lt;/a&gt;, and commented on how, apart
from automatic measures, they use human-driven evaluation. They
previously used human-driven relative ranking but now they resort to
direct assessment (human evaluator assigns a mark between 0 and 100 with
their judgement about the degree in which the translation adequately expresses
the meaning of the reference translation, without ever seeing the source
sentence).&lt;/p&gt;

&lt;p&gt;A piece of research that had already caught my attention and that was presented
at EAMT was &lt;a href=&#34;https://arxiv.org/abs/1804.06189&#34; target=&#34;_blank&#34;&gt;Investigating Backtranslation in Neural Machine Translation&lt;/a&gt;.
It explores the use of &lt;strong&gt;back-translation&lt;/strong&gt; as a means to improve translation
performance. Back-translation consists in: you want to train a language
direction &lt;em&gt;a ➝ b&lt;/em&gt; and you don&amp;rsquo;t have much parallel data but you have a lot
of monolingual data in the target language &lt;em&gt;b&lt;/em&gt;, so what you do is
training an auxiliary translation system on the &lt;em&gt;b ➝ a&lt;/em&gt; direction, and you create a synthetic
parallel corpus by using your auxiliary system on your monolingual data.
You now add your synthetic data to the original parallel data you had an
&lt;em&gt;voilà&lt;/em&gt;, you can train your &lt;em&gt;a ➝ b&lt;/em&gt; system on a lot of real + synthetic parallel data.
This is a practice that most groups and companies seem to be applying nowadays, as
evidenced by the
&lt;a href=&#34;http://matrix.statmt.org/?mode=all&#34; target=&#34;_blank&#34;&gt;WMT 2018 news translation task system brief descriptions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My company, &lt;a href=&#34;http://www.lucysoftware.com/&#34; target=&#34;_blank&#34;&gt;Lucy Software&lt;/a&gt;,
has been devising &lt;strong&gt;Rule-based Machine Translation (RBMT)&lt;/strong&gt; systems for over 30 years.
To my delight, at EAMT I got the confirmation that RBMT
systems are still relevant nowadays, especially for low-resource
&lt;a href=&#34;https://www.quora.com/When-is-a-language-said-to-be-morphologically-rich&#34; target=&#34;_blank&#34;&gt;morphologically-rich&lt;/a&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Agglutinative_language&#34; target=&#34;_blank&#34;&gt;agglutinative&lt;/a&gt; languages with
&lt;a href=&#34;https://en.wikipedia.org/wiki/Word_order&#34; target=&#34;_blank&#34;&gt;flexible word order&lt;/a&gt;, like
&lt;a href=&#34;https://en.wikipedia.org/wiki/Basque_language&#34; target=&#34;_blank&#34;&gt;Basque&lt;/a&gt;,
&lt;a href=&#34;https://en.wikipedia.org/wiki/Turkish_language&#34; target=&#34;_blank&#34;&gt;Turkish&lt;/a&gt;
or &lt;a href=&#34;https://en.wikipedia.org/wiki/Kazakh_language&#34; target=&#34;_blank&#34;&gt;Kazakh&lt;/a&gt;.
My main line of research at Lucy consists in hybridizing RBMT and
NMT to leverage the large amount of formalized linguistic knowledge in our
rule-based systems in the form of monolingual and bilingual lexicons and grammars
for source language analysis, source to target transfer and target language generation.
Confirming that the linguistic knowledge in rule-based
systems is crucial for some language pairs encourages me to pursue my RBMT-NMT
hybrid and to test it on such type of languages.&lt;/p&gt;

&lt;p&gt;As a side note, be aware that using BLEU to assess the performance of an
RBMT system in comparison with SMT or NMT is unfair. This
quality evaluation measure compares &lt;em&gt;ngram-wise&lt;/em&gt; the translation against a
reference human translation, normally taken
from a holdout set from a larger training corpus. The translations by SMT and NMT systems
are expected to be similar to the reference. RBMT translations are not necessarily so
similar to the references. Therefore, comparing RBMT against SMT and NMT on the basis
of BLEU scores is not a sensible practice. Reviewers
of the world, please take this into account when you ask article authors for BLEU scores.&lt;/p&gt;

&lt;p&gt;In the last years, the work of many human translators has shifted from pure translation
to &lt;strong&gt;post-edition of machine translations&lt;/strong&gt; due to the the increasing
quality in machine translations. However, post-edition work is much less
&lt;em&gt;rewarding&lt;/em&gt; to them, and it is paid less.
It is no secret that those human translators are
worried about how MT may affect their work in the future,
and even fearing that its quality may reach the point where there is
no space for human translators anymore.
With the newly introduced &lt;em&gt;translators track&lt;/em&gt;, this year EAMT
conference gave the machine translation community the opportunity to
interact with human translators to get direct insight on how they
feel about that and to try to find better synergic workflows
involving MT and human translators. One of the conclusions of the
translators was specially interesting: as NMT translations are very very
fluent, post-editors need to pay more attention (i.e. devote more time) than with
SMT not to overlook problems in the translated sentence.&lt;/p&gt;

&lt;h2 id=&#34;random-pictures-of-posters&#34;&gt;Random Pictures of Posters&lt;/h2&gt;

&lt;p&gt;Finally, these are some posters that got my attention at the conference:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/eamt2018/euskera.jpg&#34; alt=&#34;Euskera&#34; /&gt;
&lt;img src=&#34;/img/eamt2018/kazakh.jpg&#34; alt=&#34;Kazazh&#34; /&gt;
&lt;img src=&#34;/img/eamt2018/toofluent.jpg&#34; alt=&#34;NMT is too fluent&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notes on ICLR 2018</title>
      <link>/post/iclr2018/</link>
      <pubDate>Sun, 28 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/iclr2018/</guid>
      <description>

&lt;p&gt;I recently attended &lt;a href=&#34;http://iclr.cc/&#34; target=&#34;_blank&#34;&gt;ICLR 2018&lt;/a&gt;, as I had a workshop article accepted.
Please, consider taking a look:
&lt;a href=&#34;https://openreview.net/forum?id=HkG7hzyvf&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;A differentiable BLEU loss. Analysis and first results&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This was the first time I
attended a conference, so I tried to learn as much as I could. These are some
random notes about how the conference works and what people talked about.&lt;/p&gt;

&lt;h2 id=&#34;how-the-conference-works&#34;&gt;How the conference works&lt;/h2&gt;

&lt;p&gt;ICLR is an &lt;em&gt;artificial intelligence&lt;/em&gt; conference that uses a
&lt;a href=&#34;https://www.kdnuggets.com/2016/02/iclr-deep-learning-scientific-publishing-experiment.html&#34; target=&#34;_blank&#34;&gt;double-blind peer review process&lt;/a&gt;
via &lt;a href=&#34;https://openreview.net&#34; target=&#34;_blank&#34;&gt;OpenReview&lt;/a&gt;. Its format has changed a lot
since its creation in &lt;a href=&#34;https://sites.google.com/site/representationlearning2013/&#34; target=&#34;_blank&#34;&gt;2013&lt;/a&gt;; here
I comment on its format for 2018.&lt;/p&gt;

&lt;p&gt;ICLR submissions are submitted under
two different &lt;em&gt;tracks&lt;/em&gt;: &lt;strong&gt;conference track&lt;/strong&gt; and &lt;strong&gt;workshop track&lt;/strong&gt;. The submissions
accepted to the conference track appear in the conference proceedings while
the conferenfe track articles are &lt;em&gt;non-archival&lt;/em&gt;. The former usually has its deadline in
late October while the Workshop track deadline is after the acceptance decissions for
the Conference track are out.&lt;/p&gt;

&lt;p&gt;Submissions to the Conference track can either be accepted,
rejected or proposed for the Workshop track (i.e. automatic acceptance at the Workshop
track if the authors present it there). Papers accepted for the Conference track
must present a &lt;strong&gt;poster&lt;/strong&gt; and the top ones also have an &lt;strong&gt;oral presentation&lt;/strong&gt;. Papers submitted
to the Workshop track can be accepted (either automatically for papers submitted to
Conference that got acceptance to Workshop, or because they were accepted as part of
the Workshop track review process) or rejected. Accepted Workshop papers must present
a poster. You can find acceptance rates from previous years
&lt;a href=&#34;https://medium.com/@karpathy/iclr-2017-vs-arxiv-sanity-d1488ac5c131&#34; target=&#34;_blank&#34;&gt;on the internet&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The conference takes place during 4 days. Each day there are some invited talks, a few
oral presentations and poster sessions (in the morning and in the afternoon).
Each accepted paper is assigned a poster session. During it, the author(s) stand next to their
poster and explain their article to attendees that show interest in it.
There are also company booths for promotion and hiring purposes.&lt;/p&gt;

&lt;h2 id=&#34;what-people-talked-about&#34;&gt;What people talked about&lt;/h2&gt;

&lt;p&gt;My research is about Neural Machine Translation and how to induce prior
linguistic knowledge into translation models. The things listed in this post
are those that caught my interest and might be very biased toward my
interests and previous experiences.&lt;/p&gt;

&lt;h3 id=&#34;language-modeling-and-machine-translation&#34;&gt;Language Modeling and Machine Translation&lt;/h3&gt;

&lt;p&gt;There were not a lot of articles in natural language processing, but there
were some very interesting ones.&lt;/p&gt;

&lt;p&gt;In my opinion, the most interesting article on Language Modeling
was &lt;a href=&#34;https://openreview.net/forum?id=HkwZSG-CZ&#34; target=&#34;_blank&#34;&gt;Breaking the Softmax Bottleneck&lt;/a&gt;,
which reformulates LM as a matrix factorisation and
identifies a that the rank of the result of the softmax
is much lower than it would be needed for language modelling, and
proposes a mixture of softmaxes that palliates such a problem
effectively. In a conversation with the authors, they expressed
their reserves in the effectiveness of their method when applied
to neural machine translation, as the matrix factorisation formulation
was not suitable in that case. This article built their LM on the sota
LM implementation from
&lt;a href=&#34;https://openreview.net/forum?id=SyyGPP0TZ&#34; target=&#34;_blank&#34;&gt;Regularizing and Optimizing LSTM Language Models&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The most remarkable articles about Neural Machine translation were in my
opinion the parallel submissions about Unsupervised Machine Translation
by the University of the Basque Country
(&lt;a href=&#34;https://openreview.net/forum?id=Sy2ogebAW&#34; target=&#34;_blank&#34;&gt;Unsupervised Neural Machine Translation&lt;/a&gt;)
and Facebook
(&lt;a href=&#34;https://openreview.net/forum?id=rkYTTf-AZ&#34; target=&#34;_blank&#34;&gt;Unsupervised Machine Translation Using Monolingual Corpora Only&lt;/a&gt;).
Both propose similar ideas: using multi-task learning with (denoising) auto-encoding and
backtranslation as subtasks, being the main difference the fact that the latter relies
on adversarial training to align internal representations while the former relies
on pre-computed cross-lingual word embeddings (also created from monolingual data).&lt;/p&gt;

&lt;h3 id=&#34;generative-adversarial-networks&#34;&gt;Generative Adversarial Networks&lt;/h3&gt;

&lt;p&gt;Since they were proposed in 2014, GANs have been widening their adoption
and increasing their momentum year after year. In ICLR 2018, despite not being a lot
of GAN papers, there were enough to make me think that GANs are going remain as
one of the main research topics in deep learning at least for a few more years.&lt;/p&gt;

&lt;p&gt;Morning oral presentations from day #1 were devoted to GANs. One of them
&lt;a href=&#34;https://twitter.com/noecasas/status/991129366858948608&#34; target=&#34;_blank&#34;&gt;impressed me&lt;/a&gt;, not only because
of its amazing results but also because of the presentation style. It was the talk by
&lt;a href=&#34;https://scholar.google.es/citations?user=-50qJW8AAAAJ&#34; target=&#34;_blank&#34;&gt;Tero Karras&lt;/a&gt; from Nvidia
about progressively growing GANs. His slides had fully automatic slide transitions
(he did not press any button) and he was giving his speech as the slides were
transitionining with perfect timing and using a was super calmed and clear discourse.&lt;/p&gt;

&lt;p&gt;The posters about GANs I liked most were
&lt;a href=&#34;https://openreview.net/forum?id=ByOExmWAb&#34; target=&#34;_blank&#34;&gt;MaskGAN&lt;/a&gt;,
&lt;a href=&#34;https://openreview.net/forum?id=rkTS8lZAb&#34; target=&#34;_blank&#34;&gt;Boundary Seeking GANs&lt;/a&gt;
and
&lt;a href=&#34;https://openreview.net/forum?id=BkeqO7x0-&#34; target=&#34;_blank&#34;&gt;Unsupervised Cipher Cracking Using Discrete GANs&lt;/a&gt;
, all of them attempting
to apply GANs to discrete sequences.&lt;/p&gt;

&lt;p&gt;Though not about GANs, there was an intriguing workshop paper proposing a technique
called &lt;a href=&#34;https://openreview.net/forum?id=BJPlJVywz&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Non-Adversarial Matching&lt;/em&gt;&lt;/a&gt;
that aims at tasks similar to those addressed with
&lt;a href=&#34;https://arxiv.org/abs/1703.10593&#34; target=&#34;_blank&#34;&gt;CycleGANs&lt;/a&gt; but without needing
adversarial training.&lt;/p&gt;

&lt;h3 id=&#34;graph-neural-networks&#34;&gt;Graph Neural Networks&lt;/h3&gt;

&lt;p&gt;Several articles where about graph neural networks, both applied to
supervised and reinforcement learning. It seems
more and more people want to exploit graph structures in their problems.
The article I liked most was &lt;a href=&#34;https://openreview.net/forum?id=rJXMpikCZ&#34; target=&#34;_blank&#34;&gt;Graph Attention Networks&lt;/a&gt;,
which proposed a self-attention mechanism over graph structures.&lt;/p&gt;

&lt;p&gt;A special mention is the application of neural networks for graph
generation in computational chemistry, like the
&lt;a href=&#34;https://openreview.net/forum?id=SyqShMZRb&#34; target=&#34;_blank&#34;&gt;Syntax-Directed Variational Autoencoder for Structured Data&lt;/a&gt;,
which externalises the generation itself to an external formal grammar.&lt;/p&gt;

&lt;h3 id=&#34;deep-rl-and-policy-gradient-methods&#34;&gt;Deep RL and Policy gradient methods&lt;/h3&gt;

&lt;p&gt;After the success of AlphaGo, the RL trend went up. However, the need
for extensive hyperparameter search made it
not permeate much to universities where you cannot afford having
hundreds of GPUs computing during weeks to train a model, even though
&lt;a href=&#34;https://deepmind.com/blog/population-based-training-neural-networks/&#34; target=&#34;_blank&#34;&gt;Population-Based Training&lt;/a&gt;
has reduced notably the amount of needed processing.&lt;/p&gt;

&lt;p&gt;An article I liked was
&lt;a href=&#34;https://openreview.net/forum?id=SyzKd1bCW&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Backpropagation through the Void: Optimizing control variates for black-box gradient estimation&lt;/em&gt;&lt;/a&gt;. It proposes an unbiased gradient estimator for stochastic nodes
in a computational graph, like
&lt;a href=&#34;http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf&#34; target=&#34;_blank&#34;&gt;REINFORCE&lt;/a&gt;
but with a control variate, with much better results than
&lt;a href=&#34;https://openreview.net/forum?id=ryBDyehOl&#34; target=&#34;_blank&#34;&gt;REBAR&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Apart from that one, there were other articles proposing strategies to reduce the variance
of gradient estimators by defining baselines and control variates, including
an &lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=393&#34; target=&#34;_blank&#34;&gt;oral presentation&lt;/a&gt;. However, one
of the most interesting articles on that topic to me was workshop paper
&lt;a href=&#34;https://openreview.net/forum?id=HyL0IKJwM&#34; target=&#34;_blank&#34;&gt;The Mirage of Action-Dependent Baselines in Reinforcement Learning&lt;/a&gt;,
which finds that RL action variance reduction algorithms from the
literature (and also some of the articles presented at ICLR 2018)
are subtly wrong in their claims, as they don&amp;rsquo;t reduce variance
but trade it for bias due to implementation bugs.
Note that this conclusion only refers to action-dependent variance
reduction.&lt;/p&gt;

&lt;h3 id=&#34;reproducibility&#34;&gt;Reproducibility&lt;/h3&gt;

&lt;p&gt;If you have done deep reinforcement learning, you probably experienced
yourself that algorithms which on paper are exceedingly performant,
present very poor convergence and are very sensible to hyperparameter
selection. In &lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=346&#34; target=&#34;_blank&#34;&gt;her invited talk&lt;/a&gt;,
&lt;a href=&#34;https://scholar.google.com/citations?user=CEt6_mMAAAAJ&#34; target=&#34;_blank&#34;&gt;Joelle Pineau&lt;/a&gt;
talked about this, with references to her previous work in
(&lt;a href=&#34;https://arxiv.org/abs/1709.06560&#34; target=&#34;_blank&#34;&gt;Henderson et al., 2017&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Problems in reproducibility of scientific claims in machine learning seem to be
not restricted only to deep RL, but to be
part of a general &lt;em&gt;reproducibility crisis&lt;/em&gt;. In order to mitigate it,
she advocated for releasing source code and being fair when reporting
results (e.g. not reporting only the best top K performant runs of
a deep RL algorotithm, as they are evidently biased).&lt;/p&gt;

&lt;p&gt;Part of her efforts to increase awareness about lack of reproducibility were poured on the
&lt;a href=&#34;https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html&#34; target=&#34;_blank&#34;&gt;ICLR 2018 Reproducibility Challenge&lt;/a&gt;
that she coordinated this year. Some of the participants engaged in useful discussions
with the authors of the articles they were trying to reproduce and led
authors to improve on the clarity of their texts.&lt;/p&gt;

&lt;h3 id=&#34;source-code-synthesis-and-related-tasks&#34;&gt;Source code synthesis and related tasks&lt;/h3&gt;

&lt;p&gt;There were several articles about source code synthesis and other tasks
related to source code, like bug detection based on graphs.
&lt;a href=&#34;https://medium.com/@ilblackdragon/program-synthesis-papers-at-iclr-2018-3d3fd3b24464&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt;
you can find a blog post devoted to this subset of ICLR 2018 papers.&lt;/p&gt;

&lt;h3 id=&#34;meta-learning-and-architecture-search&#34;&gt;Meta-learning and Architecture Search&lt;/h3&gt;

&lt;p&gt;It is totally unrelated to my research area, but I had the impression that,
despite being very incipient yet,
the interest in meta-learning and architecture search has increased a lot and
that it is going to become a hot topic in the upcoming years.&lt;/p&gt;

&lt;h2 id=&#34;random-pictures-of-posters&#34;&gt;Random Pictures of Posters&lt;/h2&gt;

&lt;p&gt;Finally, this is a random selection of pictures of posters I took at the conference:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/iclr2018/cypherGAN.jpg&#34; alt=&#34;CypherGAN&#34; /&gt;
&lt;img src=&#34;/img/iclr2018/baselines.jpg&#34; alt=&#34;Mirage of action-dependent baselines&#34; /&gt;
&lt;img src=&#34;/img/iclr2018/graphattention.jpg&#34; alt=&#34;Graph Attention Nets&#34; /&gt;
&lt;img src=&#34;/img/iclr2018/latentconstraints.jpg&#34; alt=&#34;Latent Constraints &#34; /&gt;
&lt;img src=&#34;/img/iclr2018/softmax.jpg&#34; alt=&#34;Softmax Bottleneck&#34; /&gt;
&lt;img src=&#34;/img/iclr2018/void.jpg&#34; alt=&#34;Backpropagation through the void&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Shameless self promotion&amp;hellip;this is my poster:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/iclr2018/myposter.jpg&#34; alt=&#34;My poster&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Deterministic Policy Gradient for Urban Traffic Light Control</title>
      <link>/publication/2017_arxiv_drl_traffic/</link>
      <pubDate>Wed, 01 Feb 2017 10:00:00 +0000</pubDate>
      
      <guid>/publication/2017_arxiv_drl_traffic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Genetic Algorithms for multimodal optimization: a review</title>
      <link>/publication/2015_arxiv_genalg_multimodal/</link>
      <pubDate>Wed, 01 Apr 2015 10:00:00 +0000</pubDate>
      
      <guid>/publication/2015_arxiv_genalg_multimodal/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
